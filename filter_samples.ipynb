{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Starting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, slr_pkg.clean_load_data as cld, slr_pkg.para as para\n",
    "from slr_pkg.clean_load_data import open_table\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# get current working directory\n",
    "bp = Path(os.getcwd())\n",
    "\n",
    "# set results directory\n",
    "results_path = bp / \"results\"\n",
    "\n",
    "# Ask for county to gather data for.\n",
    "area = input('Enter county: ')\n",
    "# area = 'Ventura'\n",
    "# Set paths of sample data.\n",
    "edf_path = bp / 'geotracker_edf_results'\n",
    "gama_path = bp / 'gama_results'\n",
    "\n",
    "# List of contaminants.\n",
    "chems = para.conts11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and Concat Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of geotracker files to open\n",
    "edf_files = edf_path.glob('**/*{}*.zip'.format(area))\n",
    "\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "edf_results_list = [cld.Sample_Data.geotracker_df(i) for i in edf_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "print('\\nConcatenating Geotracker EDF results: \\n')\n",
    "try:\n",
    "    edf_results = pd.concat(edf_results_list)\n",
    "except:\n",
    "    edf_results = edf_results_list[0]\n",
    "\n",
    "print('Geotracker EDF results: \\n')\n",
    "print(edf_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of GAMA files to open.\n",
    "gama_files = gama_path.glob('**/*{}*.zip'.format(area.lower()))\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "gama_results_list = [cld.Sample_Data.gama_df(i) for i in gama_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe.\n",
    "print('\\nConcatenating gama results: \\n')\n",
    "gama_results = pd.concat(gama_results_list)\n",
    "\n",
    "print(\"GAMA results: \")\n",
    "print(gama_results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = cld.Sample_Data.concat_samples(edf_results, gama_results)\n",
    "print(\"Samples: \\n\")\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GEO XY PATH\n",
    "geo_xy_path = bp / 'geotracker_xy'\n",
    "\n",
    "\n",
    "def create_geo_xy(p):  # simple function for loading gama tables\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape',\n",
    "                            quotechar='\"',  quoting=3,  on_bad_lines='warn')\n",
    "\n",
    "        df['WID'] = df['GLOBAL_ID'] + '-' + df['FIELD_PT_NAME']\n",
    "        columns = ['WID', 'LATITUDE', 'LONGITUDE']\n",
    "        df = df[columns]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        print('Exception, no such file.')\n",
    "\n",
    "\n",
    "def concat_geo_xy(files):  # function to concat gama result datasets\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_geo_xy(i)\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "\n",
    "    concatDF = pd.concat(df_list, axis=0)\n",
    "\n",
    "    for df in df_list:\n",
    "        del df\n",
    "\n",
    "    return concatDF\n",
    "\n",
    "\n",
    "geo_xy_files = geo_xy_path.glob('**/*.zip')\n",
    "print('Loading Geotracker XY \\n')\n",
    "geo_xy = concat_geo_xy(geo_xy_files)\n",
    "\n",
    "# geo_xy_gpd = gpd.GeoDataFrame(geo_xy, geometry=gpd.points_from_xy(geo_xy.LONGITUDE, geo_xy.LATITUDE), crs='EPSG:4326')\n",
    "\n",
    "# load GAMA XY\n",
    "print('Loading GAMA XY \\n')\n",
    "gama_xy_path = bp / \"gama_xy\\gama_location_construction_v2.zip\"\n",
    "gama_xy = pd.read_table(gama_xy_path, sep='\\t', encoding='unicode_escape')\n",
    "\n",
    "gama_xy.rename(columns={'GM_WELL_ID': 'WID', 'GM_LATITUDE': 'LATITUDE',\n",
    "                'GM_LONGITUDE': 'LONGITUDE'}, inplace=True)\n",
    "gama_xy_columns = ['WID', 'LATITUDE', 'LONGITUDE']\n",
    "gama_xy = gama_xy[gama_xy_columns]\n",
    "\n",
    "# combine well location data into singular dataset\n",
    "print('Combining GAMA and Geotracker XY \\n')\n",
    "wells = pd.concat([gama_xy, geo_xy], ignore_index=True)\n",
    "wells = wells.drop_duplicates(subset='WID').dropna(subset=['LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Well Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join well location data to sample results.\n",
    "samples = samples.merge(wells, left_on='WID', right_on='WID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Conversion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversion tables.\n",
    "metric_conversion = pd.read_excel(bp / 'unit_conversion.xlsx', sheet_name='metric')\n",
    "molar_conversion = pd.read_excel(bp / 'unit_conversion.xlsx', sheet_name='molar')\n",
    "\n",
    "# join coversion factors to samples based on sample unit.\n",
    "samples = samples.merge(metric_conversion, how='inner', left_on='UNITS', right_on='start_unit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **All Samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join MCL Table to Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading MCL table \\n')\n",
    "\n",
    "# Create path to mcl table.\n",
    "mcl_path = bp / 'MCLs.xlsx'\n",
    "\n",
    "# Open mcl table.\n",
    "mcl = pd.read_excel(mcl_path,sheet_name='MCL', engine='openpyxl')\n",
    "\n",
    "# join MCL values to sample results\n",
    "print('Joining MCL values to samples \\n')\n",
    "samples = samples.merge(mcl, left_on='PARLABEL', right_on='chem_abrv', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for samples with MCL units in UG/L and converts sample result units to UG/L.\n",
    "mask = samples['units'] == 'UG/L'\n",
    "\n",
    "# Multiply sample results by conversion factor.\n",
    "samples.loc[mask, 'PARVAL'] = samples['PARVAL'] * samples['coef']\n",
    "\n",
    "# Drop columns that are not needed.\n",
    "samples.drop(columns=['start_unit', 'coef'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Exceedence and Magnitude Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exceedence attribute, true if sample result exceeds reporting limit.\n",
    "samples['exceedence'] = samples['PARVAL'] > samples['comp_conc_val']\n",
    "\n",
    "# Create magnitude attribute. Sample result value divided by the comparison concentration value (MCL or Action level) minus 1.\n",
    "samples['magnitude'] = (samples['PARVAL'] / samples['comp_conc_val']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples.to_csv(results_path / '{}_all_sample_results.csv'.format(area.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Specific Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select samples taken since 2010.\n",
    "samples = samples.loc[samples['LOGDATE'] >= '2012-01-01']\n",
    "\n",
    "samples = samples.loc[samples['PARLABEL'].isin(chems)]\n",
    "\n",
    "# Create groups of samples based on WID and PARLABEL(contaminant label).\n",
    "sample_groups = samples.groupby(['WID'])['PARLABEL'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def select_wells(row):\n",
    "    vals = row.values[1]\n",
    "    counter = Counter(vals)\n",
    "    #print(counter)\n",
    "    if len(counter) == len(chems):\n",
    "        if all(values >= 4 for values in counter.values()) == True:\n",
    "            print('True')\n",
    "            print(row.values[0])\n",
    "            print(counter.values())\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Create mask of sample groups meeting parameter requirements.\n",
    "res = sample_groups.apply(select_wells, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mask to select sample results from wells that meet parameter requirements.\n",
    "samples = samples[samples['WID'].isin(sample_groups.loc[res, 'WID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique wells\n",
    "nwells = len(samples['WID'].unique())\n",
    "print('Number of wells: ' + str(nwells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample results to csv.\n",
    "samples.to_csv(results_path / '{}_spec_sample_results_11.csv'.format(area.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Groundwater Elevations to Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depth to Water Data\n",
    "# Load GAMA dtw data\n",
    "# Create elev_path.\n",
    "elev_path = bp / 'elevation'\n",
    "print(elev_path, '\\n')\n",
    "\n",
    "# Dictionary of data types for gama_elev gama_elev for open_table().\n",
    "gama_elev_dtypes = {\n",
    "    'WELL NUMBER' : 'string',\n",
    "    'DEPTH TO WATER' : 'float64',\n",
    "    }\n",
    "\n",
    "# Date column of gama_elev gama_elev for open_table().\n",
    "gama_elev_date = ['MEASUREMENT DATE']\n",
    "\n",
    "# Columns of gama_elev gama_elev for open_table().\n",
    "gama_elev_cols = list(gama_elev_dtypes.keys()) + gama_elev_date\n",
    "\n",
    "\n",
    "print('Loading GAMA groundwater elevations. \\n')\n",
    "\n",
    "# create list of files to open\n",
    "gama_elev_files = elev_path.glob('**/*gama*.zip')\n",
    "gama_elev_files = list(gama_elev_files)\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "gama_elev_list = [open_table(i,dtypes = gama_elev_dtypes,date_cols = gama_elev_date, cols =gama_elev_cols) for i in gama_elev_files]\n",
    "print(gama_elev_list)\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "if len(gama_elev_list) > 1:\n",
    "    gama_elev = pd.concat(gama_elev_list)\n",
    "\n",
    "else:\n",
    "    gama_elev = gama_elev_list[0]\n",
    "\n",
    "# Dict of attributes to rename.\n",
    "gama_geo_dict = {\n",
    "    'WELL NUMBER' : 'WID',\n",
    "    'DEPTH TO WATER' : 'DTW',\n",
    "    'MEASUREMENT DATE' : 'LOGDATE',\n",
    "}\n",
    "# Rename columns.\n",
    "gama_elev = gama_elev.rename(columns=gama_geo_dict)\n",
    "\n",
    "# Fix column formatting.\n",
    "gama_elev['LOGDATE'] = gama_elev['LOGDATE'].astype(str)\n",
    "gama_elev['WID'] = gama_elev['WID'].str.replace(' ', '')\n",
    "\n",
    "# Create GID (group id) column. GID is the WID and LOGDATE concatenated.\n",
    "gama_elev['GID'] = list(zip(gama_elev['WID'], gama_elev['LOGDATE']))\n",
    "# Load Geotracker DTW data.\n",
    "# Dictionary of data types for geo_elev geo_elev for open_table().\n",
    "geo_elev_dtypes = {\n",
    "    'GLOBAL_ID' : 'string',\n",
    "    'FIELD_POINT_NAME' : 'string',\n",
    "    'DTW' : 'float64',\n",
    "    }\n",
    "\n",
    "# Date column of geo_elev geo_elev for open_table().\n",
    "geo_elev_date = ['GW_MEAS_DATE']\n",
    "\n",
    "# Columns of geo_elev geo_elev for open_table().\n",
    "geo_elev_cols = list(geo_elev_dtypes.keys()) + geo_elev_date\n",
    "\n",
    "print('Loading Geotracker groundwater elevations. \\n')\n",
    "\n",
    "# create list of files to open\n",
    "geo_elev_files = elev_path.glob('**/*Geo*.zip')\n",
    "geo_elev_files = list(geo_elev_files)\n",
    "\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "geo_elev_list = [open_table(i,geo_elev_dtypes,date_cols= geo_elev_date,cols =geo_elev_cols) for i in geo_elev_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "if len(geo_elev_list) > 1:\n",
    "    geo_elev = pd.concat(geo_elev_list)\n",
    "\n",
    "else:\n",
    "    geo_elev = geo_elev_list[0]\n",
    "\n",
    "# Create WID column.\n",
    "geo_elev['WID'] = geo_elev['GLOBAL_ID'] + '-' + geo_elev['FIELD_POINT_NAME']\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "geo_elev = geo_elev.drop(columns=['GLOBAL_ID', 'FIELD_POINT_NAME'])\n",
    "\n",
    "# fix column formatting.\n",
    "geo_elev['WID'] = geo_elev['WID'].str.replace(' ', '')\n",
    "\n",
    "# Rename columns.\n",
    "geo_elev = geo_elev.rename(columns={'GW_MEAS_DATE' : 'LOGDATE'})\n",
    "\n",
    "# Fix column formatting.\n",
    "geo_elev['LOGDATE'] = geo_elev['LOGDATE'].astype(str)\n",
    "\n",
    "# Create GID (group id) column. GID is the WID and LOGDATE concatenated.\n",
    "geo_elev['GID'] = list(zip(geo_elev['WID'], geo_elev['LOGDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gama_results and edf_results.\n",
    "dtw = pd.concat([geo_elev, gama_elev])\n",
    "\n",
    "# List of columns that require a value.\n",
    "dtw_req_cols = ['WID','DTW','LOGDATE']\n",
    "\n",
    "# Drops rows with missing values in required columns.\n",
    "dtw = dtw.dropna(subset=dtw_req_cols)\n",
    "\n",
    "# Drop duplicate GID rows.\n",
    "dtw = dtw.drop_duplicates(subset=['GID'])\n",
    "samples_dtw = samples.merge(dtw, left_on=['GID'], right_on=['GID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = pd.concat([geo_elev, gama_elev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dtw.columns\n",
    "\n",
    "dtw_req_cols = ['LOGDATE_x', 'PARLABEL', 'PARVAL', 'WID_x', 'DTW', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "for i in samples_dtw.columns:\n",
    "    if i not in dtw_req_cols:\n",
    "        samples_dtw = samples_dtw.drop(columns=i)\n",
    "\n",
    "print(samples_dtw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_num = str(len(chems))\n",
    "\n",
    "a = (len(samples))\n",
    "b = (len(samples_dtw))\n",
    "c =((len(samples_dtw) / len(samples)*100))\n",
    "c = \"{:.2f}\".format(c)\n",
    "\n",
    "print(area, chem_num,': \\n')\n",
    "print('There are ' + str(b) + ' samples with depth to water values.')\n",
    "print(\"Out of \" + str(a) + \" samples in the original dataframe.\")\n",
    "print(str(c) + \"% of samples. \\n\")\n",
    "\n",
    "a = (len(samples['WID'].unique()))\n",
    "b = (len(samples_dtw['WID_x'].unique()))\n",
    "c = (b/a)*100\n",
    "c = \"{:.2f}\".format(c)\n",
    "\n",
    "print('There are ' + str(b) + ' wells with depth to water values.')\n",
    "print(\"Out of \" + str(a) + \" wells in the original dataframe.\")\n",
    "print(str(c) + \"% of  wells. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample results to csv.\n",
    "samples_dtw.to_csv(results_path / '{}_sample_results_{}.csv'.format(area.lower(), chem_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table for CCME Water Quality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results.rename(columns={'WID' : 'Station', 'LOGDATE' : 'Date'}, inplace=True)\n",
    "\n",
    "sample_results['PARLABEL'] = sample_results['PARLABEL'] + '_' + sample_results['units']\n",
    "\n",
    "pivot_table = pd.pivot_table(sample_results, index=['Station', 'Date'], columns=['PARLABEL'], values=['PARVAL'])\n",
    "ccme_wqi_data = pivot_table.reset_index()\n",
    "\n",
    "ccme_wqi_data.columns = ['Station', 'Date', 'AS_UG/L', 'BZME_UG/L', 'BZ_UG/L', 'CD_UG/L', 'DBCP_UG/L',\n",
    "       'EBZ_UG/L', 'EDB_UG/L', 'MTBE_UG/L', 'NO3N_MG/L', 'PB_UG/L', 'PCE_UG/L',\n",
    "       'TCE_UG/L', 'TCPR123_UG/L', 'THM_UG/L', 'XYLENES_UG/L']\n",
    "\n",
    "ccme_wqi_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccme_wqi_data.to_csv(results_path / '{}_ccme_wqi_conc_samples.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Sample Result Values at Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the mean of magnitudes for each WID in the exceedences dataframe.\n",
    "print('Calculating magnitudes for each WID \\n')\n",
    "print(samples_mcl.head())\n",
    "\n",
    "means = samples_mcl.groupby(['WID'])['magnitude'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join mean magnitudes to well locations.\n",
    "print('Merging geometric mean magnitudes to wells \\n')\n",
    "wells = wells.merge(means, how='inner', left_on='WID', right_index=True)\n",
    "wells = wells.set_index('WID').sort_index()\n",
    "\n",
    "# Save well mean magnitudes to csv.\n",
    "wells.to_csv(bp / 'wells.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert well mean magnitudes to shapefile\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create geodataframe from well mean magnitudes, uses long and lat columns as xy coordinates, NAD83 projection.\n",
    "gdf = gpd.GeoDataFrame(wells, geometry=gpd.points_from_xy(x=wells.LONGITUDE, y=wells.LATITUDE), crs='EPSG:4326')\n",
    "\n",
    "# Reproject to UTM 11N.\n",
    "gdf = gdf.to_crs('EPSG:26911')\n",
    "\n",
    "\n",
    "gdf.to_file(results_path / 'wells.shp'.format(county))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sample Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group samples by WID and LOGDATE apply list function to get list of PARLABELS for each group.\n",
    "sample_groups = samples_mcl.groupby(['WID', 'LOGDATE'])['PARLABEL'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Contaminant List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list comprehension to create a list of sample indexes where all contaminants in the contaminant list are present.\n",
    "index_list = [i for i in sample_groups.index if all(item in sample_groups.loc[i] for item in contaminants_3)]\n",
    "\n",
    "# Uses index_list to create a dataframe of samples that meet the criteria.\n",
    "sample_group_results = samples_mcl[samples_mcl['GID'].isin(index_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groups of samples that meet the criteria.\n",
    "print('Groups: ',len(index_list))\n",
    "print('Samples: ',len(sample_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join location data to sample results.\n",
    "sample_group_results = sample_results.merge(wells, left_on='WID', right_on='WID', how='inner')\n",
    "\n",
    "# Save sample group results to csv.\n",
    "sample_group_results.to_csv(bp / '{}_sample_results.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results.to_csv(bp / '{}_sample_results.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contaminant Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Create list of all combinations of all contaminants\n",
    "combinations_list = list(combinations(contaminants_3, 10))\n",
    "len(combinations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select sample groups based on combinations of contaminants.\n",
    "def get_select_samples(row, contaminants):\n",
    "\n",
    "    # checks list of contaminants in row against list of contaminants in function call.\n",
    "    # if all contaminants in row are in contaminants, return True.\n",
    "    if all(item in row for item in contaminants):\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_select_samples(row, contaminants):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for values in row:\n",
    "        print(values)\n",
    "        if all(item in values for item in contaminants):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_dict = {}\n",
    "\n",
    "total =  len(combinations_list)\n",
    "count = 0\n",
    "\n",
    "for contaminants in combinations_list:\n",
    "\n",
    "    count += 1\n",
    "    percent = int(((count/total)*100))\n",
    "\n",
    "    ser = sample_groups.apply(get_select_samples, contaminants=contaminants_3)\n",
    "\n",
    "    ser_dict[contaminants] = ser\n",
    "\n",
    "    #print('{}%'.format(percent))\n",
    "    \n",
    "combo_stats = pd.DataFrame.from_dict(ser_dict, orient='index')\n",
    "\n",
    "print(combo_stats.max())\n",
    "print(list(combo_stats.idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modin Combo Stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_select_samples_modin(row, contaminants):\n",
    "    print(row)\n",
    "\n",
    "    if all(element in row for element in contaminants) ==  True:\n",
    "        print('contains all elements')\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print('does not contain all elements')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_modin = mpd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "\n",
    "sample_groups_modin = mpd.DataFrame(sample_groups)\n",
    "\n",
    "\n",
    "ser_dict = {}\n",
    "\n",
    "for contaminants in combinations_list:\n",
    "\n",
    "    ser = sample_groups_modin.apply(get_select_samples_modin, contaminants=contaminants)\n",
    "    ser = ser[ser == True]\n",
    "\n",
    "    ser_dict[contaminants] = len(ser)\n",
    "\n",
    "\n",
    "\n",
    "print(max(ser_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combo_stats.max())\n",
    "print(list(combo_stats.idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading MCL table \\n')\n",
    "\n",
    "# Create path to mcl table.\n",
    "mcl_path = bp / 'MCL_list_1.xlsx'\n",
    "\n",
    "# Open mcl table.\n",
    "mcl = pd.read_excel(mcl_path, engine='openpyxl')\n",
    "\n",
    "# join MCL values to sample results\n",
    "print('Joining MCL values to samples \\n')\n",
    "samples_mcl = select_samples.merge(mcl, left_on='PARLABEL', right_on='chem_abrv', how='left').set_index(select_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples_mcl to csv.\n",
    "alt = input(\"Input filename ending for 'county'_select_samples_'input'.csv: \")\n",
    "name = '{}_select_samples_{}.csv'.format(county.lower(), alt)\n",
    "sp = bp / name\n",
    "samples_mcl.to_csv(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of samples for each contaminant.\n",
    "parlabel_stats = samples['PARLABEL'].value_counts()\n",
    "\n",
    "# Create a dataframe with the counts of samples for each contaminant.\n",
    "parlabel_stats = parlabel_stats.to_frame(name='COUNTS').reset_index().rename(columns={'index':'PARLABEL'})\n",
    "\n",
    "# Create PERCENT column for each contaminant. Showing percent of samples for each contaminant compared to total samples.\n",
    "parlabel_stats['PERCENT'] = (parlabel_stats['COUNTS'] / len(samples) * 100).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples_mcl to csv.\n",
    "name = '{}_parlabel_stats.csv'.format(county.lower())\n",
    "sp = bp / name\n",
    "parlabel_stats.to_csv(sp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23234625f55973f7a58126a35d86facfdbb1213f4cf262be4a4984331c60271a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('geoprj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
