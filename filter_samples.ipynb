{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages and Set Starting Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get current working directory\n",
    "bp = Path(os.getcwd())\n",
    "\n",
    "# set results directory\n",
    "results_path = bp / \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for county to gather data for.\n",
    "county = input('Enter county: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of contaminants from CES Drinking Water Quality index plux BTEX and MTBE.\n",
    "contaminants_2 = [\n",
    "    'AS',\n",
    "    'BZ',\n",
    "    'BZME',\n",
    "    'CD',\n",
    "    'CR6',\n",
    "    'DBCP',\n",
    "    'EBZ',\n",
    "    'EDB',\n",
    "    'HAA5',\n",
    "    'NO3N',\n",
    "    'MTBE',\n",
    "    'PB',\n",
    "    'PCATE',\n",
    "    'PCE',\n",
    "    'TCE',\n",
    "    'TCPR123',\n",
    "    'THM',\n",
    "    'XYLENES',\n",
    "    ]\n",
    "\n",
    "# Selected contaminants\n",
    "contaminants_3 = [\n",
    "    'BZ', \n",
    "    'BZME', \n",
    "    'DBCP', \n",
    "    'EBZ', \n",
    "    'EDB',  \n",
    "    'MTBE', \n",
    "    'PCE', \n",
    "    'TCE', \n",
    "    'TCPR123', \n",
    "    'XYLENES', \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Open and Concat Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open data file.\n",
    "\"\"\"\n",
    "open_table() is a function that opens a csv file and returns a dataframe. \n",
    "Will try to open the file with the default encoding, if that fails, will try with the unicode_escape encoding.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "Args:\n",
    "    p: path to file\n",
    "    dtypes: dictionary of data types\n",
    "    date_cols: list of columns to parse as dates\n",
    "    cols: list of columns to use\n",
    "\"\"\"\n",
    "\n",
    "def open_table(p, dtypes, date_cols, cols):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(p, sep='\\t', dtype=dtypes, on_bad_lines='warn', parse_dates=date_cols, usecols=cols)\n",
    "        return df\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep='\\t', dtype=dtypes, on_bad_lines='warn', encoding='unicode_escape', parse_dates=date_cols, usecols=cols)\n",
    "            return df\n",
    "            \n",
    "        except:\n",
    "            df = pd.read_csv(p, sep='\\t', dtype=dtypes, engine='python', encoding='unicode_escape', on_bad_lines='warn', parse_dates=date_cols, usecols=cols)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geotracker path.\n",
    "edf_path = bp / 'geotracker_edf_results'\n",
    "\n",
    "# Dictionary of data types for geotracker edf_results for open_table().\n",
    "geotracker_dtypes = {\n",
    "    'GLOBAL_ID' : 'string',\n",
    "    'FIELD_PT_NAME' : 'string',\n",
    "    'PARLABEL' : 'string',\n",
    "    'PARVAL' : 'Float64',\n",
    "    'PARVQ' : 'string',\n",
    "    'REPDL' : 'Float64',\n",
    "    'UNITS' : 'string',\n",
    "    }\n",
    "\n",
    "# Date column of geotracker edf_results for open_table().\n",
    "geotracker_date = ['LOGDATE']\n",
    "\n",
    "# Columns of geotracker edf_results for open_table().\n",
    "geotracker_cols = list(geotracker_dtypes.keys()) + geotracker_date\n",
    "\n",
    "print('Loading Geotracker EDF results: \\n')\n",
    "\n",
    "# create list of files to open\n",
    "edf_files = edf_path.glob('**/*{}*.zip'.format(county))\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "edf_results_list = [open_table(i,geotracker_dtypes,geotracker_date,geotracker_cols) for i in edf_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "print('Concatenating EDF results: \\n')\n",
    "if len(edf_results_list) > 1:\n",
    "    edf_results = pd.concat(edf_results_list)\n",
    "\n",
    "else:\n",
    "    edf_results = edf_results_list[0]\n",
    "\n",
    "# Create WID column.\n",
    "edf_results['WID'] = edf_results['GLOBAL_ID'] + '-' + edf_results['FIELD_PT_NAME']\n",
    "edf_results['WID'] = edf_results['WID'].str.replace(' ','')\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "edf_results = edf_results.drop(columns=['GLOBAL_ID', 'FIELD_PT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path of gama_results.\n",
    "gama_path = bp / 'gama_results'\n",
    "\n",
    "# Dictionary of data types for gama_results for open_table().\n",
    "gama_dtypes = {\n",
    "    'GM_WELL_ID' : 'string',\n",
    "    'GM_CHEMICAL_VVL' : 'string',\n",
    "    'GM_RESULT_MODIFIER' : 'string',\n",
    "    'GM_RESULT' : 'Float64',\n",
    "    'GM_RESULT_UNITS' : 'string',\n",
    "    'GM_REPORTING_LIMIT' : 'Float64',\n",
    "    }\n",
    "\n",
    "# Date column of gama_results for open_table().\n",
    "gama_date = ['GM_SAMP_COLLECTION_DATE']\n",
    "\n",
    "# Columns of gama_results for open_table().\n",
    "gama_cols = list(gama_dtypes.keys()) + gama_date\n",
    "\n",
    "print('Loading GAMA results: \\n')\n",
    "\n",
    "# Create list of files to open.\n",
    "gama_files = gama_path.glob('**/*{}*.zip'.format(county.lower()))\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "\n",
    "gama_results_list = [open_table(i,gama_dtypes,gama_date,gama_cols) for i in gama_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe.\n",
    "print('Concatenating gama results: \\n')\n",
    "gama_results = pd.concat(gama_results_list)\n",
    "\n",
    "# Dictionary to rename gama columns to match edf_results.\n",
    "gama_to_edf_dict = {\n",
    "    'GM_WELL_ID' : 'WID',\n",
    "    'GM_CHEMICAL_VVL' : 'PARLABEL',\n",
    "    'GM_RESULT_MODIFIER' : 'PARVQ',\n",
    "    'GM_RESULT' : 'PARVAL',\n",
    "    'GM_RESULT_UNITS' : 'UNITS',\n",
    "    'GM_REPORTING_LIMIT' : 'REPDL',\n",
    "    'GM_SAMP_COLLECTION_DATE' : 'LOGDATE',\n",
    "}\n",
    "\n",
    "# Rename gama columns to match edf_results.\n",
    "gama_results = gama_results.rename(columns=gama_to_edf_dict)\n",
    "\n",
    "gama_results['WID'] = gama_results['WID'].str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gama_results and edf_results.\n",
    "samples = pd.concat([edf_results, gama_results])\n",
    "\n",
    "# List of columns that require a value.\n",
    "samples_req_cols = ['WID','LOGDATE', 'PARLABEL', 'PARVAL']\n",
    "\n",
    "# Drops rows with missing values in required columns.\n",
    "samples = samples.dropna(subset=samples_req_cols)\n",
    "\n",
    "# Set multi index on WID and LOGDATE.\n",
    "#samples = samples.set_index(['WID', 'LOGDATE']).sort_index()\n",
    "samples['LOGDATE'] = samples['LOGDATE'].astype(str)\n",
    "samples['GID'] = list(zip(samples['WID'], samples['LOGDATE']))\n",
    "samples = samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GEO XY PATH\n",
    "geo_xy_path = bp / 'geotracker_xy'\n",
    "\n",
    "\n",
    "def create_geo_xy(p):  # simple function for loading gama tables\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape',\n",
    "                            quotechar='\"',  quoting=3,  on_bad_lines='warn')\n",
    "\n",
    "        df['WID'] = df['GLOBAL_ID'] + '-' + df['FIELD_PT_NAME']\n",
    "        columns = ['WID', 'LATITUDE', 'LONGITUDE']\n",
    "        df = df[columns]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        print('Exception, no such file.')\n",
    "\n",
    "\n",
    "def concat_geo_xy(files):  # function to concat gama result datasets\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_geo_xy(i)\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "\n",
    "    concatDF = pd.concat(df_list, axis=0)\n",
    "\n",
    "    for df in df_list:\n",
    "        del df\n",
    "\n",
    "    return concatDF\n",
    "\n",
    "\n",
    "geo_xy_files = geo_xy_path.glob('**/*.zip')\n",
    "print('Loading Geotracker XY \\n')\n",
    "geo_xy = concat_geo_xy(geo_xy_files)\n",
    "\n",
    "# geo_xy_gpd = gpd.GeoDataFrame(geo_xy, geometry=gpd.points_from_xy(geo_xy.LONGITUDE, geo_xy.LATITUDE), crs='EPSG:4326')\n",
    "\n",
    "# load GAMA XY\n",
    "print('Loading GAMA XY \\n')\n",
    "gama_xy_path = bp / \"gama_xy\\gama_location_construction_v2.zip\"\n",
    "gama_xy = pd.read_table(gama_xy_path, sep='\\t', encoding='unicode_escape')\n",
    "\n",
    "gama_xy.rename(columns={'GM_WELL_ID': 'WID', 'GM_LATITUDE': 'LATITUDE',\n",
    "                'GM_LONGITUDE': 'LONGITUDE'}, inplace=True)\n",
    "gama_xy_columns = ['WID', 'LATITUDE', 'LONGITUDE']\n",
    "gama_xy = gama_xy[gama_xy_columns]\n",
    "\n",
    "# combine well location data into singular dataset\n",
    "print('Combining GAMA and Geotracker XY \\n')\n",
    "wells = pd.concat([gama_xy, geo_xy], ignore_index=True)\n",
    "wells = wells.drop_duplicates(subset='WID').dropna(subset=['LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Conversion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversion tables.\n",
    "metric_conversion = pd.read_excel(bp / 'unit_conversion.xlsx', sheet_name='metric')\n",
    "molar_conversion = pd.read_excel(bp / 'unit_conversion.xlsx', sheet_name='molar')\n",
    "\n",
    "# join coversion factors to samples based on sample unit.\n",
    "samples = samples.merge(metric_conversion, how='inner', left_on='UNITS', right_on='start_unit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **All Samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join MCL Table to Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select samples of selected contaminants \n",
    "#samples = samples[samples['PARLABEL'].isin(contaminants_3)]\n",
    "\n",
    "print('Loading MCL table \\n')\n",
    "\n",
    "# Create path to mcl table.\n",
    "mcl_path = bp / 'MCLs.xlsx'\n",
    "\n",
    "# Open mcl table.\n",
    "mcl = pd.read_excel(mcl_path,sheet_name='MCL', engine='openpyxl')\n",
    "\n",
    "# join MCL values to sample results\n",
    "print('Joining MCL values to samples \\n')\n",
    "samples_mcl = samples.merge(mcl, left_on='PARLABEL', right_on='chem_abrv', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for samples with MCL units in UG/L and converts sample result units to UG/L.\n",
    "mask = samples_mcl['units'] == 'UG/L'\n",
    "\n",
    "# Multiply sample results by conversion factor.\n",
    "samples_mcl.loc[mask, 'PARVAL'] = samples_mcl['PARVAL'] * samples_mcl['coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join well location data to sample results.\n",
    "sample_results = samples_mcl.merge(wells, left_on='WID', right_on='WID', how='inner')\n",
    "\n",
    "# Drop columns that are not needed.\n",
    "sample_results.drop(columns=['GID', 'start_unit', 'coef', 'UNITS', 'GID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exceedence attribute, true if sample result exceeds reporting limit.\n",
    "sample_results['exceedence'] = sample_results['PARVAL'] > sample_results['comp_conc_val']\n",
    "\n",
    "# Create magnitude attribute. Sample result value divided by the comparison concentration value (MCL or Action level) minus 1.\n",
    "sample_results['magnitude'] = (sample_results['PARVAL'] / sample_results['comp_conc_val']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results.to_csv(results_path / '{}_all_sample_results.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Specific Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select samples taken since 2010.\n",
    "sample_results = sample_results.loc[sample_results['LOGDATE'] >= '2010-01-01']\n",
    "\n",
    "# Create groups of samples based on WID and PARLABEL(contaminant label).\n",
    "sample_groups = sample_results.groupby(['WID'])['PARLABEL'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def select_wells(row):\n",
    "    print(vals)\n",
    "    vals = row.values[1]\n",
    "    counter = Counter(vals)\n",
    "    print(counter)\n",
    "    if len(counter) == len(contaminants_3):\n",
    "        if all(values >= 4 for values in counter.values()) == True:\n",
    "            print('True')\n",
    "            print(row.values[0])\n",
    "            print(counter.values())\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Create mask of sample groups meeting parameter requirements.\n",
    "res = sample_groups.apply(select_wells, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mask to select sample results from wells that meet parameter requirements.\n",
    "select_samples_results = sample_results[sample_results['WID'].isin(sample_groups.loc[res, 'WID'])]\n",
    "\n",
    "# Create exceedence attribute, true if sample result exceeds reporting limit.\n",
    "select_samples_results['exceedence'] = select_samples_results['PARVAL'] > select_samples_results['comp_conc_val']\n",
    "\n",
    "# Create magnitude attribute. Sample result value divided by the comparison concentration value (MCL or Action level) minus 1.\n",
    "select_samples_results['magnitude'] = (select_samples_results['PARVAL'] / select_samples_results['comp_conc_val']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique wells\n",
    "nwells = len(select_samples_results['WID'].unique())\n",
    "print('Number of wells: ' + str(nwells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample results to csv.\n",
    "select_samples_results.to_csv(results_path / '{}_spec_sample_results_10.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table for CCME Water Quality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results.rename(columns={'WID' : 'Station', 'LOGDATE' : 'Date'}, inplace=True)\n",
    "\n",
    "sample_results['PARLABEL'] = sample_results['PARLABEL'] + '_' + sample_results['units']\n",
    "\n",
    "pivot_table = pd.pivot_table(sample_results, index=['Station', 'Date'], columns=['PARLABEL'], values=['PARVAL'])\n",
    "ccme_wqi_data = pivot_table.reset_index()\n",
    "\n",
    "ccme_wqi_data.columns = ['Station', 'Date', 'AS_UG/L', 'BZME_UG/L', 'BZ_UG/L', 'CD_UG/L', 'DBCP_UG/L',\n",
    "       'EBZ_UG/L', 'EDB_UG/L', 'MTBE_UG/L', 'NO3N_MG/L', 'PB_UG/L', 'PCE_UG/L',\n",
    "       'TCE_UG/L', 'TCPR123_UG/L', 'THM_UG/L', 'XYLENES_UG/L']\n",
    "\n",
    "ccme_wqi_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccme_wqi_data.to_csv(results_path / '{}_ccme_wqi_conc_samples.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Sample Result Values at Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the mean of magnitudes for each WID in the exceedences dataframe.\n",
    "print('Calculating magnitudes for each WID \\n')\n",
    "print(samples_mcl.head())\n",
    "\n",
    "means = samples_mcl.groupby(['WID'])['magnitude'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join mean magnitudes to well locations.\n",
    "print('Merging geometric mean magnitudes to wells \\n')\n",
    "wells = wells.merge(means, how='inner', left_on='WID', right_index=True)\n",
    "wells = wells.set_index('WID').sort_index()\n",
    "\n",
    "# Save well mean magnitudes to csv.\n",
    "wells.to_csv(bp / 'wells.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert well mean magnitudes to shapefile\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create geodataframe from well mean magnitudes, uses long and lat columns as xy coordinates, NAD83 projection.\n",
    "gdf = gpd.GeoDataFrame(wells, geometry=gpd.points_from_xy(x=wells.LONGITUDE, y=wells.LATITUDE), crs='EPSG:4326')\n",
    "\n",
    "# Reproject to UTM 11N.\n",
    "gdf = gdf.to_crs('EPSG:26911')\n",
    "\n",
    "\n",
    "gdf.to_file(results_path / 'wells.shp'.format(county))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sample Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group samples by WID and LOGDATE apply list function to get list of PARLABELS for each group.\n",
    "sample_groups = samples_mcl.groupby(['WID', 'LOGDATE'])['PARLABEL'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Contaminant List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list comprehension to create a list of sample indexes where all contaminants in the contaminant list are present.\n",
    "index_list = [i for i in sample_groups.index if all(item in sample_groups.loc[i] for item in contaminants_3)]\n",
    "\n",
    "# Uses index_list to create a dataframe of samples that meet the criteria.\n",
    "sample_group_results = samples_mcl[samples_mcl['GID'].isin(index_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groups of samples that meet the criteria.\n",
    "print('Groups: ',len(index_list))\n",
    "print('Samples: ',len(sample_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join location data to sample results.\n",
    "sample_group_results = sample_results.merge(wells, left_on='WID', right_on='WID', how='inner')\n",
    "\n",
    "# Save sample group results to csv.\n",
    "sample_group_results.to_csv(bp / '{}_sample_results.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Groundwater Elevations to Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create elev_path.\n",
    "elev_path = bp / 'elevation'\n",
    "print(elev_path, '\\n')\n",
    "\n",
    "# Dictionary of data types for gama_elev gama_elev for open_table().\n",
    "gama_elev_dtypes = {\n",
    "    'WELL NUMBER' : 'string',\n",
    "    'DEPTH TO WATER' : 'float64',\n",
    "    }\n",
    "\n",
    "# Date column of gama_elev gama_elev for open_table().\n",
    "gama_elev_date = ['MEASUREMENT DATE']\n",
    "\n",
    "# Columns of gama_elev gama_elev for open_table().\n",
    "gama_elev_cols = list(gama_elev_dtypes.keys()) + gama_elev_date\n",
    "\n",
    "\n",
    "print('Loading GAMA groundwater elevations. \\n')\n",
    "\n",
    "# create list of files to open\n",
    "gama_elev_files = elev_path.glob('**/*gama*.zip')\n",
    "gama_elev_files = list(gama_elev_files)\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "gama_elev_list = [open_table(i,gama_elev_dtypes,gama_elev_date,gama_elev_cols) for i in gama_elev_files]\n",
    "print(gama_elev_list)\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "if len(gama_elev_list) > 1:\n",
    "    gama_elev = pd.concat(gama_elev_list)\n",
    "\n",
    "else:\n",
    "    gama_elev = gama_elev_list[0]\n",
    "\n",
    "# Dict of attributes to rename.\n",
    "gama_geo_dict = {\n",
    "    'WELL NUMBER' : 'WID',\n",
    "    'DEPTH TO WATER' : 'DTW',\n",
    "    'MEASUREMENT DATE' : 'LOGDATE',\n",
    "}\n",
    "# Rename columns.\n",
    "gama_elev = gama_elev.rename(columns=gama_geo_dict)\n",
    "\n",
    "# Fix column formatting.\n",
    "gama_elev['LOGDATE'] = gama_elev['LOGDATE'].astype(str)\n",
    "gama_elev['WID'] = gama_elev['WID'].str.replace(' ', '')\n",
    "\n",
    "# Create GID (group id) column. GID is the WID and LOGDATE concatenated.\n",
    "gama_elev['GID'] = list(zip(gama_elev['WID'], gama_elev['LOGDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of data types for geo_elev geo_elev for open_table().\n",
    "geo_elev_dtypes = {\n",
    "    'GLOBAL_ID' : 'string',\n",
    "    'FIELD_POINT_NAME' : 'string',\n",
    "    'DTW' : 'float64',\n",
    "    }\n",
    "\n",
    "# Date column of geo_elev geo_elev for open_table().\n",
    "geo_elev_date = ['GW_MEAS_DATE']\n",
    "\n",
    "# Columns of geo_elev geo_elev for open_table().\n",
    "geo_elev_cols = list(geo_elev_dtypes.keys()) + geo_elev_date\n",
    "\n",
    "print('Loading Geotracker groundwater elevations. \\n')\n",
    "\n",
    "# create list of files to open\n",
    "geo_elev_files = elev_path.glob('**/*Geo*.zip')\n",
    "geo_elev_files = list(geo_elev_files)\n",
    "\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "geo_elev_list = [open_table(i,geo_elev_dtypes,geo_elev_date,geo_elev_cols) for i in geo_elev_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "if len(geo_elev_list) > 1:\n",
    "    geo_elev = pd.concat(geo_elev_list)\n",
    "\n",
    "else:\n",
    "    geo_elev = geo_elev_list[0]\n",
    "\n",
    "# Create WID column.\n",
    "geo_elev['WID'] = geo_elev['GLOBAL_ID'] + '-' + geo_elev['FIELD_POINT_NAME']\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "geo_elev = geo_elev.drop(columns=['GLOBAL_ID', 'FIELD_POINT_NAME'])\n",
    "\n",
    "# fix column formatting.\n",
    "geo_elev['WID'] = geo_elev['WID'].str.replace(' ', '')\n",
    "\n",
    "# Rename columns.\n",
    "geo_elev = geo_elev.rename(columns={'GW_MEAS_DATE' : 'LOGDATE'})\n",
    "\n",
    "# Fix column formatting.\n",
    "geo_elev['LOGDATE'] = geo_elev['LOGDATE'].astype(str)\n",
    "\n",
    "# Create GID (group id) column. GID is the WID and LOGDATE concatenated.\n",
    "geo_elev['GID'] = list(zip(geo_elev['WID'], geo_elev['LOGDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gama_results and edf_results.\n",
    "dtw = pd.concat([geo_elev, gama_elev])\n",
    "\n",
    "# List of columns that require a value.\n",
    "dtw_req_cols = ['WID','DTW','LOGDATE']\n",
    "\n",
    "# Drops rows with missing values in required columns.\n",
    "dtw = dtw.dropna(subset=dtw_req_cols)\n",
    "\n",
    "# Drop duplicate GID rows.\n",
    "dtw = dtw.drop_duplicates(subset=['GID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_samples_results['GID'] = list(zip(select_samples_results['WID'], select_samples_results['LOGDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_samples_results_dtw = select_samples_results.merge(dtw, left_on=['GID'], right_on=['GID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (len(select_samples_results))\n",
    "b = (len(select_samples_results_dtw))\n",
    "c =((len(select_samples_results_dtw) / len(select_samples_results)*100))\n",
    "\n",
    "print('There are ' + str(b) + ' samples with depth to water values.')\n",
    "print(\"Out of \" + str(a) + \" samples in the original dataframe.\")\n",
    "print(\"That's \" + str(c) + \"% of the samples. \\n\")\n",
    "\n",
    "a = (len(select_samples_results['WID'].unique()))\n",
    "b = (len(select_samples_results_dtw['WID_x'].unique()))\n",
    "c = ((len(select_samples_results_dtw['WID_x'].unique()) / len(select_samples_results['WID'].unique())*100))\n",
    "\n",
    "print('There are ' + str(b) + ' wells with depth to water values.')\n",
    "print(\"Out of \" + str(a) + \" wells in the original dataframe.\")\n",
    "print(\"That's \" + str(c) + \"% of the wells. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results.to_csv(bp / '{}_sample_results.csv'.format(county.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contaminant Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Create list of all combinations of all contaminants\n",
    "combinations_list = list(combinations(contaminants_3, 10))\n",
    "len(combinations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select sample groups based on combinations of contaminants.\n",
    "def get_select_samples(row, contaminants):\n",
    "\n",
    "    # checks list of contaminants in row against list of contaminants in function call.\n",
    "    # if all contaminants in row are in contaminants, return True.\n",
    "    if all(item in row for item in contaminants):\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_select_samples(row, contaminants):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for values in row:\n",
    "        print(values)\n",
    "        if all(item in values for item in contaminants):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_dict = {}\n",
    "\n",
    "total =  len(combinations_list)\n",
    "count = 0\n",
    "\n",
    "for contaminants in combinations_list:\n",
    "\n",
    "    count += 1\n",
    "    percent = int(((count/total)*100))\n",
    "\n",
    "    ser = sample_groups.apply(get_select_samples, contaminants=contaminants_3)\n",
    "\n",
    "    ser_dict[contaminants] = ser\n",
    "\n",
    "    #print('{}%'.format(percent))\n",
    "    \n",
    "combo_stats = pd.DataFrame.from_dict(ser_dict, orient='index')\n",
    "\n",
    "print(combo_stats.max())\n",
    "print(list(combo_stats.idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modin Combo Stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_select_samples_modin(row, contaminants):\n",
    "    print(row)\n",
    "\n",
    "    if all(element in row for element in contaminants) ==  True:\n",
    "        print('contains all elements')\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print('does not contain all elements')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_modin = mpd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "\n",
    "sample_groups_modin = mpd.DataFrame(sample_groups)\n",
    "\n",
    "\n",
    "ser_dict = {}\n",
    "\n",
    "for contaminants in combinations_list:\n",
    "\n",
    "    ser = sample_groups_modin.apply(get_select_samples_modin, contaminants=contaminants)\n",
    "    ser = ser[ser == True]\n",
    "\n",
    "    ser_dict[contaminants] = len(ser)\n",
    "\n",
    "\n",
    "\n",
    "print(max(ser_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combo_stats.max())\n",
    "print(list(combo_stats.idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading MCL table \\n')\n",
    "\n",
    "# Create path to mcl table.\n",
    "mcl_path = bp / 'MCL_list_1.xlsx'\n",
    "\n",
    "# Open mcl table.\n",
    "mcl = pd.read_excel(mcl_path, engine='openpyxl')\n",
    "\n",
    "# join MCL values to sample results\n",
    "print('Joining MCL values to samples \\n')\n",
    "samples_mcl = select_samples.merge(mcl, left_on='PARLABEL', right_on='chem_abrv', how='left').set_index(select_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples_mcl to csv.\n",
    "alt = input(\"Input filename ending for 'county'_select_samples_'input'.csv: \")\n",
    "name = '{}_select_samples_{}.csv'.format(county.lower(), alt)\n",
    "sp = bp / name\n",
    "samples_mcl.to_csv(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of samples for each contaminant.\n",
    "parlabel_stats = samples['PARLABEL'].value_counts()\n",
    "\n",
    "# Create a dataframe with the counts of samples for each contaminant.\n",
    "parlabel_stats = parlabel_stats.to_frame(name='COUNTS').reset_index().rename(columns={'index':'PARLABEL'})\n",
    "\n",
    "# Create PERCENT column for each contaminant. Showing percent of samples for each contaminant compared to total samples.\n",
    "parlabel_stats['PERCENT'] = (parlabel_stats['COUNTS'] / len(samples) * 100).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples_mcl to csv.\n",
    "name = '{}_parlabel_stats.csv'.format(county.lower())\n",
    "sp = bp / name\n",
    "parlabel_stats.to_csv(sp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23234625f55973f7a58126a35d86facfdbb1213f4cf262be4a4984331c60271a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('geoprj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
