{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "bp = Path(os.getcwd())\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of contaminants to search for.\n",
    "# List of GAMA 'top 10 contaminants'\n",
    "contaminants_1 = [\n",
    "    'TCPR123',\n",
    "    'DBCP',\n",
    "    'AS',\n",
    "    'CR6',\n",
    "    'PCATE',\n",
    "    'PCE',\n",
    "    'TCE',\n",
    "    'U',\n",
    "    'BZ',\n",
    "    'BZME',\n",
    "    'EBZ',\n",
    "    'XYLENES',\n",
    "    ]\n",
    "\n",
    "# List of contaminants from CES Drinking Water Quality index plux BTEX and MTBE.\n",
    "contaminants_2 = [\n",
    "    'AS',\n",
    "    'BZ',\n",
    "    'BZME',\n",
    "    'CD',\n",
    "    'CR6',\n",
    "    'DBCP',\n",
    "    'EBZ',\n",
    "    'EDB',\n",
    "    'NO3N',\n",
    "    'PB',\n",
    "    'PCATE',\n",
    "    'PCE',\n",
    "    'TCE',\n",
    "    'TCPR123',\n",
    "    'THM',\n",
    "    'XYLENES',\n",
    "    'HAA5',\n",
    "    'MTBE',\n",
    "    ]\n",
    "    \n",
    "# List of test contaminants.\n",
    "contaminants_3 = ['AS', 'BZ', 'BZME', 'CD']\n",
    "\n",
    "# List of only BTEX contaminants.\n",
    "btex_list = [\n",
    "    'BZ',\n",
    "    'BZME',\n",
    "    'EBZ',\n",
    "    'XYLENES',\n",
    "    ]\n",
    "\n",
    "# Ask for county to gather data for.\n",
    "county = input('Enter county: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "open_table() is a function that opens a csv file and returns a dataframe. \n",
    "Will try to open the file with the default encoding, if that fails it will try with unicode_escape encoding.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "Args:\n",
    "    p: path to file\n",
    "    dtypes: dictionary of data types\n",
    "    date_cols: list of columns to parse as dates\n",
    "    cols: list of columns to use\n",
    "\"\"\"\n",
    "\n",
    "def open_table(p, dtypes, date_cols, cols):\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', dtype=dtypes, parse_dates=date_cols, usecols=cols)\n",
    "        return df\n",
    "        \n",
    "    except:\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', dtype=dtypes, parse_dates=date_cols, usecols=cols, encoding='unicode_escape')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edf_path = Path(r\"C:\\Users\\chief\\Desktop\\sample_filtering\\geotracker_edf_results\")\n",
    "edf_path = bp / 'geotracker_edf_results'\n",
    "\n",
    "# Dictionary of data types for geotracker edf_results for open_table().\n",
    "geotracker_dtypes = {\n",
    "    'GLOBAL_ID' : 'string',\n",
    "    'FIELD_PT_NAME' : 'string',\n",
    "    'PARLABEL' : 'string',\n",
    "    'PARVAL' : 'Float64',\n",
    "    'PARVQ' : 'string',\n",
    "    'REPDL' : 'Float64',\n",
    "    'UNITS' : 'string',\n",
    "    }\n",
    "\n",
    "# Date column of geotracker edf_results for open_table().\n",
    "geotracker_date = ['LOGDATE']\n",
    "\n",
    "# Columns of geotracker edf_results for open_table().\n",
    "geotracker_cols = list(geotracker_dtypes.keys()) + geotracker_date\n",
    "\n",
    "print('Loading Geotracker EDF results \\n')\n",
    "\n",
    "# create list of files to open\n",
    "edf_files = edf_path.glob('**/*{}*.zip'.format(county))\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "edf_results_list = [open_table(i,geotracker_dtypes,geotracker_date,geotracker_cols) for i in edf_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe if there are more than one.\n",
    "if len(edf_results_list) > 1:\n",
    "    edf_results = pd.concat(edf_results_list)\n",
    "\n",
    "else:\n",
    "    edf_results = edf_results_list[0]\n",
    "\n",
    "# Create WID column.\n",
    "edf_results['WID'] = edf_results['GLOBAL_ID'] + '-' + edf_results['FIELD_PT_NAME']\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "edf_results = edf_results.drop(columns=['GLOBAL_ID', 'FIELD_PT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path of gama_results.\n",
    "gama_path = bp / 'gama_results'\n",
    "\n",
    "# Dictionary of data types for gama_results for open_table().\n",
    "gama_dtypes = {\n",
    "    'GM_WELL_ID' : 'string',\n",
    "    'GM_CHEMICAL_VVL' : 'string',\n",
    "    'GM_RESULT_MODIFIER' : 'string',\n",
    "    'GM_RESULT' : 'Float64',\n",
    "    'GM_RESULT_UNITS' : 'string',\n",
    "    'GM_REPORTING_LIMIT' : 'Float64',\n",
    "    }\n",
    "\n",
    "# Date column of gama_results for open_table().\n",
    "gama_date = ['GM_SAMP_COLLECTION_DATE']\n",
    "\n",
    "# Columns of gama_results for open_table().\n",
    "gama_cols = list(gama_dtypes.keys()) + gama_date\n",
    "\n",
    "print('Loading GAMA results \\n')\n",
    "\n",
    "# Create list of files to open.\n",
    "gama_files = gama_path.glob('**/*{}*.zip'.format(county.lower()))\n",
    "\n",
    "# Use list comprehension to create a list of dataframes from the files list. Uses open_table() to open the files.\n",
    "gama_results_list = [open_table(i,gama_dtypes,gama_date,gama_cols) for i in gama_files]\n",
    "\n",
    "# Concatenate the list of dataframes into one dataframe.\n",
    "gama_results = pd.concat(gama_results_list)\n",
    "\n",
    "# Dictionary to rename gama columns to match edf_results.\n",
    "gama_to_edf_dict = {\n",
    "    'GM_WELL_ID' : 'WID',\n",
    "    'GM_CHEMICAL_VVL' : 'PARLABEL',\n",
    "    'GM_RESULT_MODIFIER' : 'PARVQ',\n",
    "    'GM_RESULT' : 'PARVAL',\n",
    "    'GM_RESULT_UNITS' : 'UNITS',\n",
    "    'GM_REPORTING_LIMIT' : 'REPDL',\n",
    "    'GM_SAMP_COLLECTION_DATE' : 'LOGDATE',\n",
    "}\n",
    "\n",
    "# Rename gama columns to match edf_results.\n",
    "gama_results = gama_results.rename(columns=gama_to_edf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gama_results and edf_results.\n",
    "samples = pd.concat([edf_results, gama_results])\n",
    "\n",
    "# List of columns that require a value.\n",
    "samples_req_cols = ['LOGDATE', 'PARLABEL', 'PARVAL']\n",
    "\n",
    "# Drops rows with missing values in required columns.\n",
    "samples = samples.dropna(subset=samples_req_cols)\n",
    "\n",
    "# Set multi index on WID and LOGDATE.\n",
    "samples = samples.set_index(['WID', 'LOGDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group samples by WID and LOGDATE apply list function to get list of PARLABELS for each group.\n",
    "sample_groups = samples.groupby(['WID', 'LOGDATE'])['PARLABEL'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list comprehension to create a list of sample indexes where all contaminants in the contaminant list are present.\n",
    "index_list = [i for i in sample_groups.index if all(item in sample_groups.loc[i] for item in contaminants_3)]\n",
    "\n",
    "# Uses index_list to create a dataframe of samples that meet the criteria.\n",
    "select_samples = samples.loc[index_list]\n",
    "\n",
    "# Keeps only samples of contaminants in the contaminant list.\n",
    "select_samples = select_samples[select_samples['PARLABEL'].isin(contaminants_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(select_samples, index=['WID', 'LOGDATE'], columns=['PARLABEL'], values=['PARVAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.to_excel(bp / 'pivot_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells.to_excel(bp / 'wells.excel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23234625f55973f7a58126a35d86facfdbb1213f4cf262be4a4984331c60271a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('geoprj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
